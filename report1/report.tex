\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amsmath}

\title{Advanced Algorithms - part 1}
\author{Arash Rouhani (rarash@student.chalmers.se) - 901117-1213}
	

\begin{document}

\maketitle

\section{Load balancing revisited}
The approximation ratio that we want to determine is $T/T^*$.

First we state the \emph{problems} lower bounds (without motivation, as they
are considered known).

$$ T^* \geq L $$
$$ T^* \geq A/m $$

Now lets consider the \emph{algorithms} upper bounds. We define $i$ and $j$
as in class: Machine $i$, $m_i$, is the machine that worked until end.
job $j$ is the last job by $m_i$, it took $t_j$ time. 

We know (again from class) that all machines worked for at least $T - t_j$.
As otherwise job $j$ would have been assigned to a different machine,
this can be stated because of "greedy".

Let $k$ be any machine, and machine $k$ totally worked for $T_k$ time.
We just said $T_k \geq T - t_j$. Using this and $\forall j. L \geq t_j$ gives

\[
 T_k \geq T - t_j \Rightarrow \sum_{k} T_k = A \geq m(T-t_j) \geq m(T-L)
\]

\[
 A \geq m(T-L) \Rightarrow A/m + L \geq T
\]

Now we have what we need in terms of upper bounds and lower bounds.
Lets use the inequalities to "solve for" $T/T^*$.

\[
 T/T^* \leq \frac{A/m + L}{A/m} = 1 + mL/A
\]

\[
  \lim_{L/A \to 0} T/T^* \leq 1 + mL/A = 1
\]

We are done.

\section{Packing Bins}

First we will give a counter example to the greedy algorithm.
After that we prove a approximation ratio for the greedy algorithm.

\subsection{Counter Example}

Setting $K = 10$ and having the input queue $5, 1, 5, 1, 5$, greedy gives
3 bins, they will contain $[5, 1] [5,1] [5]$ in that order. However the optimal
solution will require only 2 bins, they must contain $[5, 5] [1, 1, 1]$.

\subsection{Proving approximation ratio}

We denote the greedy and optimal answer as $B$ and $B^*$ respectively.
Our goal is to prove that $\lim_{n \to \infty} B/B^* \leq 2$. As before we
start studying the problems lower bounds and then the algorithms upper bounds.

A lower bound comes from the observation that in an ideal setting you would
let each bin be full, that is each bin holds $K$.

\[
 B^* * K \geq \sum w_i \Rightarrow B^* \geq \frac{\sum w_i}{K}
\]

Before venturing after upper bounds for the greedy algorithm, we define some
notation.
Let ${b_i}$ be bucket $i$ and ${b_i}_j$ be item $j$ in ${b_i}$.
Let $w({b_i})$ be the sum of weights in ${b_i}$.
Let $w_2({b_i}) = \frac{w({b_i}) + w(b_{i+1})}{2}$.

A relation we state for $w()$ and $w_2()$ is that when $n \to \infty$ (then
also $B \to \infty$)
\[
 \sum_{i=1}^{B} w_2({b_i}) = \sum_{i=1}^{B} w({b_i})
\]

Another relation between item weights $w_i$ and bin sums $w()$ that always holds
is

\[
 \sum_{i=1}^{n} w_i = \sum_{i=1}^{B} w({b_i})
\]

Above formula is common sense as all items must be in the bins exactly once.

For a given bin ${b_i}$ that has a successor $b_{i+1}$, I claim that
$w({b_i}) + {b_{i+1}}_1 \geq K$. This must hold by proof of contradiction.
The greedy algorithm will keep putting items in a bin if it has capacity for it.
Bin $i$ could have
held another item if $w({b_i}) + {b_{i+1}}_1 \leq K$. That contradicts the greedy
algorithm behavior.

Obviously we have ${b_{i+1}}_1 \leq w(b_{i+1})$, this gives

\[
 2w_2({b_i}) = w({b_i}) + w(b_{i+1}) \geq w({b_i}) + {b_{i+1}}_1 \geq K
 \Rightarrow w_2({b_i}) \geq K/2
\]

We can finally conclude a upper bound that holds when $n \to \infty$.

\[
 \sum_{i=1}^{n} w_i = \sum_{i=1}^{B} w({b_i}) = \sum_{i=1}^{B} w_2({b_i}) \geq B*K/2
\]

Now lets use our inequalities, keeping in mind that the UB requires the
constraint $n \to \infty$.

\[
 2B^* \geq 2 \frac{\sum w_i}{K} \geq 2 \frac{B*K/2}{K} = B
\]

Hence $2B^* \geq B \Rightarrow B/B^* \leq 2 $ when $n \to \infty$.
This is what we wanted to prove, so we are done.


\section{Probability theory}
To keep this reasonably dense, I answer one question for each paragraph.

$P(A \wedge B) = P(A) * P(B)$ Does not always hold, as if $A=B$ then that would
wrongly imply $P(A) = P(A)^2$. However, if $A$ and $B$ are
independent, that is $P(A \wedge B)/P(B) = P(A)$, then we immediately see
that the stated equation holds in general. The definition can be interpreted
as $A$ and $B$ are independent iff
"Ratio of $A \wedge B$ in $B$ is as big as the ratio of $A$ in $universe$".

Thinking of a $A$ and $B$ as sets, we already know that 
$|A \cup B| \leq |A| + |B|$. With the exact same reasoning
$P(A \vee B) \leq P(A) + P(B)$ will hold in general.

A random variable, often denoted $X$, is a variable that can be
different values according to some distribution.
A RV is defined differently for discrete and continuous case.
Discrete $X$ is associated with the \emph{probability function} f
such that $P(X = a) = f(a)$. When $X$ is continuous we call f a
\emph{density function} and it should then obey $f(a) = 0$ yet
$P(a < X < b) = \int_a^b f(x)dx$.

In the discrete case $E[X] = \sum_x f(x)x$, and as always the continuous
counterpart has an integral instead of a sum. If sampling $X$ a lot,
spawning $X_1, X_2, ...$ then $E[X]$ is the average of those values.

Before describing $Z = X + Y$, let's redefine a RV. $X : \Omega \to R$
is a RV, so $X(\omega) = r$. $P(X = r) = \sum_{X(\omega)=r} P(\omega)$.
Finally we can easily denote $Z(w) = (X + Y)(w) = X(w) + Y(w)$
($w$ looks like $\omega$ but easier to write),
that is the definition of
addition. Multiplication is analogous.
Before contuing, we might also want to note that
$E[X] = \sum_x P(X = x)*x  = \sum_w P(w)*X(w)$.

Yes for both 3.6 and 3.7. But 3.7 only holds for independent variables.
I have attached proofs for both 3.6 and 3.7, the proofs are for
the discrete case, but will likely be analogous in the continuous case.

\begin{align*}
 E[X+Y] &= \sum_w P(w)*(X+Y)(w) \\
        &= \text{\{split the sums\}} \\
        &= E[Y] + E[X]\\
\\
\\
 E[X*Y] &= \sum_x \sum_y P(X = x \wedge Y = y)(x*y) \\
        &= \text{\{independence\}} \\
        &= \sum_x \sum_y f_X(x)f_Y(y)(x*y) \\
        &= \sum_x f_X(x)x \sum_y f_Y(y)y \\
        &= E[Y]\sum_x f_X(x)x \\
        &= E[Y] * E[X]
\end{align*}

\end{document}
