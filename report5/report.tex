\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{pdfpages}

\title{Advanced Algorithms - part 5 (exc 12-13)}
\author{Arash Rouhani (rarash@student.chalmers.se) - 901117-1213}

\begin{document}

\maketitle

\section{"eat one's cake and have it too" combined algorithm}

I propose an algorithm that runs both the algorithms in parallel,
and returns the answer as soon as any algorithm terminates.
We assign different priorities to the algorithms depending on
the parameter $s$.

The first objection might be \emph{why paralell}? Why not just
one of them? Will it not always be faster to just pick one of them?
Yes! But we don't know which one as of the uncertainty introduced
by the randomized algorithm.
Our randomized algorithm
doesn't have a predictable end, if we run the randomized algorithm
on its own we get no $O(g(n))$ bound. Running only the deterministic
algorithm will not have expected time $s*f(n)$.

So we need both algorithms and also in parallel.
But how do we ensure expected time $s*f(n)$, well
that we obviously get if the "thread" running the randomized
algorithm would have a slowdown of $s$, that is it get
$1/s$ of the attention in the parallel execution.
From this we get that the deterministic "thread" get
$1-1/s$ attention. Here we have assumed
perfect parallelism.

Ok, can we show that this new algorithm both is $O(g(n))$ time and
$s*f(n)$ expected running time? As for the expected running time
we know it is $s*f(n)$ as we choose the $1/s$ factor for
the randomized algorithm just for this reason. Note that if
we have insanely chosen $s$ to be large we evetually
get the purpose-defeating inequality $s*f(n) > g(n)$.
In that case the expected running time would be $g(n)$ or better.
As for if the algorithm is $O(g(n))$. Yes, as the running
time in the worst case is when the deterministic algorithm terminates
before the randomized, yielding running time of

\[
\frac{1}{1-1/s}g(n) = c(s)*g(n) \in O(g(n))
\]

Since $c(s)$ is a constant with respect to $n$ the
algorithm is $O(g(n))$.

\section{Family of colorings}

\textbf{13.1:}
Note: I have not solved task 13.1, and I can't give
a correct solution, but I think its better to write
something than nothing. The solution lead
to a better bound than expected.

Let first note that there are ${n \choose k}$ subsets,
and we order them in some way so we can index them with $j$.
Let $C_i$ be a random coloring, and let $X_i$ be the number
of colorful subsets generated by $C_i$. Also let
$X_{ij} = 1$ iff $C_i$ colorfully paints $S_j$, $0$ ow.

What is $E[X_{ij}]$? Of course it is independent for $i$ and $j$.

\[
E[X_{ij}] = k!/k^k
\]

Now of course $X_i = \sum_j X_{ij}$, by linearity of expectation
we have

\[
E[X_i] = {n \choose k} * E[X_{ij}] = \frac{n!}{(n-k)!k^k}
\]

Now lets decide we want $m$ colorings, that is $ 0 < i \leq m$.
Also we introduce $X = \sum_i X_i$. This means that $X$ is the total
number of subsets that a coloring colors colorfully,
\emph{with duplicates}.

Anyway I had ideas to express $E[X] = m E[X_i]$, then we know
that $P(X > E[X]) > 0$, now, what is good about $m$ is that
we can assign $E[X] := {n \choose k}$ and configure $m$ to satisfy
that. With the assigned $E[X]$ we have $P(X > {n \choose k}) > 0$
which is exactly what we want and uses the probabilisitic method.

With all this hope up I saw to my disappointment that
$m = k^k/k! = O(e^k)$, which is better than the correct answer.
I first thought this was due to counting of duplicates in $X$, but
now I find that unlikely. Because it is always possible to
"rearrange" a coloring that is mostly colorfully painting
an already "done" subset. By rearranging you could get a coloring
that colors "undone" subsets instead. Of course random colorings will
likely overlap and cause duplicately counted subsets,
but the probabilistic method is about if something
is possible, not if something is likely.

\textbf{13.2:} Yes, we can use it but is it good?
First I will say \emph{how} and then I will compare the
algorithm to say if it is good compared to other.

Recall from the lecture notes of the last lecture that
if we were looking for a path with cardinality $k$,
and that if we had a coloring that would colorfully
paint a $k$-length-path $P$, we have a $O(2^kn^2)$ DP-algorithm
that would find $P$. Now we create an algorithm using this
DP-algorithm.
\textbf{Algorithm:} For each
coloring $f \in F$, run the DP-algorithm with
with $f$, eventually we will find a solution.  \textbf{End.}
Any node subset $S$ of cardinality $k$
have at least one coloring $f \in F$ that paints $S$ colorfully,
since the optimal path $P$ consists of the nodes for some subset $S$,
that path will thus be checked by the DP-algorithm. The analysis
is trivial, time complexity is
$O(ke^klog(n))*O(2^kn^2) = O(k(2k)^kn^2log(n))$ in worst case.
Worst case is that the last coloring $f$ we check is the
one that yields a colorful coloring for $S$, where $S$ is the
nodes in the path $P$ of length $k$, and we only have one such path.

How does this compare to the algorithm in class?
Time complexity was $O((2e)^kn^2)$ estimated time,
it did not have the two additional factors $k$ and $log(n)$.
So, is it faster than the "family algorithm" presentend above?
Yes, but the randomized algorithm presented in class gives no
worst case bound, only an estimated time, and in fact it will
after a while return that no solution exists if it doesn't find
one. For some applications that is unacceptable, that is when
you need $100\%$ certainty wheter if a solution exists or not.
On the other hand you are often satisfied with a faster and
very likely to succeed algorithm.

In conclusion, $F$ can be used to solve the problem.
The algorithm has a worst case bound and is good for some
applications. However most often you are better of with
the randomized algorithm from class which is slightly faster,
but can not guarantee that it will find a solution.

Note: If you are \emph{really} going for the gold, you
can get the best from both worlds by applying the solution
from Excercise 12.

\textbf{13.3:} Imagine we have created a hash table.
At the time of creation we knew which $k$ elements
that are "insertable". So this hash table can not insert
other elements than the $k$ pre-defined elements.
These $k$ elements can be any subset $S$ from a larger pool
with $n$ elements. From the family $F$ we extract the function
$f : ElementSet \to ColorSet$ that satisfies $k=|S|=|f(S)|$.
In words, we pick the coloring
from $F$ that paints $S$ uniquely, we call that coloring $f$.
We know such function $f$ exists as of the definition of $F$.

The hashing scheme is simply to use a restrictned
version of $f$ which we can call $h$. $h : S \to ColorSet$
and $h(e) = f(e)$. $h$ will for any of the $k$ elements
give a unique color/memcell, thus being an ideal hash
function for the set $S$.

In conclusion, yes we can get a has scheme by extracting
$f \in F$ which yields a hasing function $h$.

\end{document}
