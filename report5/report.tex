\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{pdfpages}

\title{Advanced Algorithms - part 5 (exc 12-13)}
\author{Arash Rouhani (rarash@student.chalmers.se) - 901117-1213}

\begin{document}

\maketitle

\section{"eat one's cake and have it too" combined algorithm}

I propose an algorithm that runs both the algorithms in parallel,
and returns the answer as soon as any algorithm terminates.
We assign different priorities to the algorithms depending on
the parameter $s$.

The first objection might be \emph{why paralell}? Why not just
one of them? Will it not always be faster to just pick one of them?
Yes! But we don't know which one as of the uncertainty introduced
by the randomized algorithm.
Our randomized algorithm
doesn't have a predictable end, if we run the randomized algorithm
on it's own we get no $O(g(n))$ bound. Running only the deterministic
algorithm will not have expected time $s*f(n)$.

So we need both algorithms and also in parallel.
But how do we ensure expected time $s*f(n)$, well
that we obviously get if the "thread" running the randomized
algorithm would have a slowdown of $s$, that is it get
$1/s$ of the attention in the parallel execution.
From this we get that the deterministic "thread" get
$1-1/s$ attention. Here we have assumed
perfect parallelism.

Ok, can we show that this new algorithm both is $O(g(n))$ time and
$s*f(n)$ expected running time? As for the expected running time
we know it is $s*f(n)$ as we choose the $1/s$ factor for
the randomized algorithm just for this reason. Note that if
we have insanely chosen $s$ to be large we evetually
get the purpose-defeating inequality $s*f(n) > g(n)$.
In that case the expected running time would be $g(n)$ or better.
As for if the algorithm is $O(g(n))$. Yes, as the running
time in the worst case is when the deterministic algorithm terminates
before the randomized, yielding running time of

\[
\frac{1}{1-1/s}g(n) = c(s)*g(n) \in O(g(n))
\]

Since $c(s)$ is a constant with respect to $n$ the
algorithm is $O(g(n))$.

\section{Family of colorings}

\textbf{13.1:}
\textbf{13.2:} Yes, we can use it but is it good?
First I will say \emph{how} and then I will compare the
algorithm to say if it is good compared to other.

Recall from the lecture notes of the last lecture that
if we were looking for a path with cardinality $k$,
and that if we had a coloring that would colorfully
paint a $k$-length-path $P$, we have a $O(2^kn^2)$ DP-algorithm
that would find $P$.
\textbf{Algorithm:} For each
coloring $f \in F$, run the DP-algorithm with
with $f$, eventually we will find a solution.\textbf{End.}
Any node subset $S$ of cardinality $k$
have at least one coloring $f \in F$ that paints $S$ colorfully,
since the optimal path $P$ consists of the nodes for some subset $S$,
that path will thus be checked by the DP-algorithm. The analysis
is trivial, time complexity is
$O(ke^klog(n))*O(2^kn^2) = O(k(2k)^kn^2log(n))$ in worst case.
Worst case is that the last coloring $f$ we check is the
one that yields a colorful coloring for $S$, where $S$ is the
nodes in the path $P$ of length $k$, and we only have one such path.

How does this compare to the algorithm in class?
Time complexity was $O((2e)^kn^2)$ estimated time,
it did not have the two additional factors $k$ and $log n$.
So, is it faster than the "family algorithm" presentend above?
Yes, but the randomized algorithm presented in class gives no
worst case bound, only an estimated time, and in fact it will
after a while return that no solution exists if it doesn't find
one. For some applications that is unacceptable, that is when
you need $100\%$ certainty wheter if a solution exists or not.
On the other hand you are often satisfied with a faster and
very likely to succeed algorithm.

In conclusion, $F$ can be used to solve the problem.
The algorithm has a worst case bound and is good for some
applications. However most often you are better of with
the randomized algorithm from class which is slightly faster,
but can not guarantee that it will find a solution.

Note: If you are \emph{really} going for the gold, you
can get the best from both worlds by applying the solution
from Excercise 12.

\textbf{13.3:} Imagine we have created a hash table,
at the time of creation we knew which $k$ elements
that are "insertable". So this hash table can not insert
other elements than the $k$ pre-defined elements.
These $k$ elements can be any subset $S$ from a larger pool
with $n$ elements. From the family $F$ we extract the function
$f : ElementSet \to ColorSet$ that satisfies $k=|S|=|f(S)|$.
In words, we pick the coloring
from $F$ that paints $S$ uniquely, we call that coloring $f$.
We know such function $f$ exists as of the definition of $F$.

The hashing scheme is simply to use a restrictned
version of $f$ which we can call $h$. $h : S \to ColorSet$
and $h(e) = f(e)$. $h$ will for any of the $k$ elements
give a unique color/memcell, thus being an ideal hash
function for the set $S$.

In conclusion, yes we can get a has scheme by extracting
$f \in F$ which yields a hasing function $h$.

\end{document}
