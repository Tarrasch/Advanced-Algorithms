\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{pdfpages}

\title{Advanced Algorithms - part 5 (exc 12-13)}
\author{Arash Rouhani (rarash@student.chalmers.se) - 901117-1213}

\begin{document}

\maketitle

\section{"eat one's cake and have it too" combined algorithm}

I propose an algorithm that runs both the algorithms in parallel,
and returns the answer as soon as any algorithm terminates.
We assign different priorities to the algorithms depending on
the parameter $s$.

The first objection might be \emph{why paralell}? Why not just
one of them? Will it not always be faster to just pick one of them?
Yes! But we don't know which one as of the uncertainty introduced
by the randomized algorithm.
Our randomized algorithm
doesn't have a predictable end, if we run the randomized algorithm
on its own we get no $O(g(n))$ bound. Running only the deterministic
algorithm will not have expected time $s*f(n)$.

So we need both algorithms and also in parallel.
But how do we ensure expected time $s*f(n)$, well
that we obviously get if the "thread" running the randomized
algorithm would have a slowdown of $s$, that is it get
$1/s$ of the attention in the parallel execution.
From this we get that the deterministic "thread" get
$1-1/s$ attention. Here we have assumed
perfect parallelism.

Ok, can we show that this new algorithm both is $O(g(n))$ time and
$s*f(n)$ expected running time? As for the expected running time
we know it is $s*f(n)$ as we choose the $1/s$ factor for
the randomized algorithm just for this reason. Note that if
we have insanely chosen $s$ to be large we evetually
get the purpose-defeating inequality $s*f(n) > g(n)$.
In that case the expected running time would be $g(n)$ or better.
As for if the algorithm is $O(g(n))$. Yes, as the running
time in the worst case is when the deterministic algorithm terminates
before the randomized, yielding running time of

\[
\frac{1}{1-1/s}g(n) = c(s)*g(n) \in O(g(n))
\]

Since $c(s)$ is a constant with respect to $n$ the
algorithm is $O(g(n))$.

\section{Family of colorings}

\textbf{13.1:}
Let a subset being "ccolored" mean that the subset is
colorfully colored by at least one coloring in the family.

First we define events $A_i$ to "happen" when the subset
$S_i$ is not ccolored. When any $A_i$ happens (when $\cup_i A_i$ happens),
it means that there is one set that isn't ccolored.
We trivially calculate $P(A_i) = (1 - p)^t$ where $p = k!/k^k \approx e^{-k}$
and $t$ is the number of colorings we have.
If we could show $P(\cup_i A_i) < 1 $ then there certainly exists a
family with the desired property, we have used the probibalistic method.
However the probability $P(\cup_i A_i)$ is difficult to calculate, luckily
the union bound gives a bound we can work with.

\[
P(\cup_i A_i) \leq \sum_i P(A_i)
              = {n \choose k}P(A_i)
              = {n \choose k}(1 - p)^t
              < 1
\]

We can now start solving for $t$. Begin with applying $\ln$.

\[
\ln n! -\ln k! -\ln (n-k)!  + t \ln(1-p) < 0
\]

Recall \emph{Stirling's approximation},
$\ln  n!  = n \ln n  - n + O(\ln n ) \approx n \ln n $.
Furthermore we will introduce $a = \ln(1-p)$ which we will handle later.

\[
a t > -(\ln n! -\ln k! -\ln((n-k)!))
    = n(\ln(n-k) - \ln n ) + k(\ln k  - \ln(n-k))
    \approx -k \ln n
    = O(k \ln n )
\]

The "$\approx$" needs some explenation, assuming $n \gg k$, we see
that $(\ln(n-k) - \ln n )$ tends to zero canceling the $n$, meanwhile
$(\ln k  - \ln(n-k)$ tends to $-\ln(n-k) \approx -\ln n $.
This factor explains the $O(k \ln n)$ part of $t=O(k e^k \ln n )$, now
only remains to show $a = O(e^{-k})$.

\[
a = \ln(1-p)
  \approx -p
  = -e^{-k}
  = O(e^{-k})
\]

We used the fact that $\ln(1+\epsilon) = \epsilon$.
And we are done.

\textbf{13.2:} Yes, we can use it but is it good?
First I will say \emph{how} and then I will compare the
algorithm to say if it is good compared to other.

Recall from the lecture notes of the last lecture that
if we were looking for a path with cardinality $k$,
and that if we had a coloring that would colorfully
paint a $k$-length-path $P$, we have a $O(2^kn^2)$ DP-algorithm
that would find $P$. Now we create an algorithm using this
DP-algorithm.
\textbf{Algorithm:} For each
coloring $f \in F$, run the DP-algorithm with
with $f$, eventually we will find a solution.  \textbf{End.}
Any node subset $S$ of cardinality $k$
have at least one coloring $f \in F$ that paints $S$ colorfully,
since the optimal path $P$ consists of the nodes for some subset $S$,
that path will thus be checked by the DP-algorithm. The analysis
is trivial, time complexity is
$O(ke^klog(n))*O(2^kn^2) = O(k(2k)^kn^2log(n))$ in worst case.
Worst case is that the last coloring $f$ we check is the
one that yields a colorful coloring for $S$, where $S$ is the
nodes in the path $P$ of length $k$, and we only have one such path.

How does this compare to the algorithm in class?
Time complexity was $O((2e)^kn^2)$ estimated time,
it did not have the two additional factors $k$ and $log(n)$.
So, is it faster than the "family algorithm" presentend above?
Yes, but the randomized algorithm presented in class gives no
worst case bound, only an estimated time, and in fact it will
after a while return that no solution exists if it doesn't find
one. For some applications that is unacceptable, that is when
you need $100\%$ certainty wheter if a solution exists or not.
On the other hand you are often satisfied with a faster and
very likely to succeed algorithm.

In conclusion, $F$ can be used to solve the problem.
The algorithm has a worst case bound and is good for some
applications. However most often you are better of with
the randomized algorithm from class which is slightly faster,
but can not guarantee that it will find a solution.

Note: If you are \emph{really} going for the gold, you
can get the best from both worlds by applying the solution
from Excercise 12.

\textbf{13.3:} Imagine we have created a hash table.
At the time of creation we knew which $k$ elements
that are "insertable". So this hash table can not insert
other elements than the $k$ pre-defined elements.
These $k$ elements can be any subset $S$ from a larger pool
with $n$ elements. From the family $F$ we extract the function
$f : ElementSet \to ColorSet$ that satisfies $k=|S|=|f(S)|$.
In words, we pick the coloring
from $F$ that paints $S$ uniquely, we call that coloring $f$.
We know such function $f$ exists as of the definition of $F$.

The hashing scheme is simply to use a restrictned
version of $f$ which we can call $h$. $h : S \to ColorSet$
and $h(e) = f(e)$. $h$ will for any of the $k$ elements
give a unique color/memcell, thus being an ideal hash
function for the set $S$.

About the common operations \emph{insertion, deletion}
and \emph{look up}. Given the fact we know which $k$ unique elements
that can interact with our hash table,
we could let the hash table simply
be a $k$ length bitstring $b = b_1 b_2 \dots b_k$ where
$b_i = 1$ iff element $e \in S$ is in the hash table
and $i = h(e)$.
\textbf{Correctness of representation:}  Since we have only $k$ elements
this bitstring can cover the presence of all those elements.
There are no duplicates and their associated color is unique,
so there can never be more than
one element at cell $i$ in the hash table. So
a binary number representing presence or non-presence
is therfor enough.
 \textbf{End.}
With this representation we can define the three operations.

\[
insert(e) \to b_{h(e)} := 1
\]
\[
delete(e) \to b_{h(e)} := 0
\]
\[
lookup(e) \to b_{h(e)} == 1
\]

One good property of these functions that even if $h$
is bijective and has an inverse, it's not neccesary
to use it to implement the operations. Also they
are $O(1)$ assuming the hash function also is.

In conclusion, yes we can get a hash scheme by extracting
$f \in F$ which yields a hashing function $h$. Then
$h$ can be used when creating a hash table.

\end{document}
